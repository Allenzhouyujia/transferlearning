{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Lesson 4: ResNet50 Transfer Learning for Flower Classification\n",
        "\n",
        "## Overview\n",
        "Learn transfer learning with ResNet50 on the Flowers102 dataset. This lesson demonstrates how deeper networks can improve performance and compares results with ResNet18 from Lesson 3.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand ResNet50 architecture and bottleneck blocks\n",
        "- Implement transfer learning with a deeper network\n",
        "- Compare performance between ResNet18 and ResNet50\n",
        "- Analyze computational trade-offs between model depth and performance\n",
        "\n",
        "### Model Quick Facts\n",
        "- **Architecture**: ResNet50 (50 layers, 25.6M parameters)\n",
        "- **Pre-training**: ImageNet dataset (1.2M images, 1000 classes)\n",
        "- **Key Innovation**: Bottleneck blocks for efficient deep networks\n",
        "- **Transfer Method**: Feature extraction + fine-tuning\n",
        "- **Expected Performance**: ~88%+ accuracy on Flowers102 (vs ~85% for ResNet18)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Environment Setup and Library Imports\n",
        "\n",
        "### Why This Step Matters\n",
        "Setting up the environment correctly is crucial for:\n",
        "- **Reproducibility**: Ensuring consistent results across different runs\n",
        "- **Performance**: Optimizing GPU usage and memory management\n",
        "- **Debugging**: Clean output without unnecessary warnings\n",
        "\n",
        "### Key Libraries Explained\n",
        "- **torch**: Core PyTorch library (tensors, automatic differentiation, neural networks)\n",
        "- **torchvision**: Computer vision utilities (datasets, transforms, pre-trained models)\n",
        "- **models**: Pre-trained model architectures (ResNet50, ResNet18, etc.)\n",
        "- **optim**: Optimization algorithms (SGD, Adam, AdamW)\n",
        "- **DataLoader**: Efficient batch processing and parallel data loading\n",
        "- **tqdm**: Progress bars for training loops\n",
        "- **matplotlib**: Data visualization and plotting\n",
        "- **sklearn**: Machine learning utilities (metrics, confusion matrix)\n",
        "\n",
        "### Configuration Settings\n",
        "We configure matplotlib for high-quality visualizations and set up proper warning filters for cleaner output during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core PyTorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Computer vision utilities\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Machine learning utilities\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for high-quality plots\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.style.use('default')\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "print(f\"ğŸ“¦ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ğŸ–¼ï¸ Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"ğŸ”¥ CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"ğŸ MPS available: {torch.backends.mps.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Device Detection and Configuration\n",
        "\n",
        "### Device Selection Strategy\n",
        "ResNet50 requires more computational resources than ResNet18. Our device detection follows this priority:\n",
        "\n",
        "1. **CUDA GPU** (NVIDIA): Highly recommended for ResNet50 training\n",
        "   - Parallel processing with thousands of cores\n",
        "   - Large memory capacity for deep networks\n",
        "   - Optimized for matrix operations\n",
        "\n",
        "2. **MPS (Apple Silicon)**: Apple's Metal Performance Shaders\n",
        "   - Efficient on M1/M2 chips\n",
        "   - May need batch size reduction for memory constraints\n",
        "   - Good performance for development\n",
        "\n",
        "3. **CPU**: Not recommended for ResNet50\n",
        "   - Very slow training (hours instead of minutes)\n",
        "   - Use only for testing/debugging\n",
        "\n",
        "### Training Configuration\n",
        "We use the same parameters as ResNet18 for fair comparison:\n",
        "- **Batch Size**: 32 (may need reduction to 16 for memory limits)\n",
        "- **Learning Rate**: 0.001 (standard for AdamW optimizer)\n",
        "- **Epochs**: 50 total (20 frozen + 30 fine-tuning)\n",
        "- **Optimizer**: AdamW with weight decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device detection with fallback hierarchy\n",
        "print(\"ğŸ” Detecting optimal compute device...\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"ğŸš€ Using NVIDIA GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"   ğŸ’¡ ResNet50 recommended: Good memory for deep network\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"ğŸ Using Apple Silicon GPU (MPS)\")\n",
        "    print(\"   Optimized for M1/M2 chips\")\n",
        "    print(\"   âš ï¸  May need batch size reduction for ResNet50\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"ğŸ’» Using CPU\")\n",
        "    print(\"   âš ï¸  NOT recommended for ResNet50 - very slow training\")\n",
        "\n",
        "# Set training configuration\n",
        "print(\"\\nâš™ï¸ Setting up training configuration...\")\n",
        "config = {\n",
        "    'batch_size': 32,  # May need reduction for memory limits\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 50,\n",
        "    'freeze_epochs': 20,\n",
        "    'finetune_epochs': 30,\n",
        "    'num_workers': 2,\n",
        "    'weight_decay': 0.01\n",
        "}\n",
        "\n",
        "print(f\"   ğŸ“¦ Batch size: {config['batch_size']} (reduce to 16 if memory issues)\")\n",
        "print(f\"   ğŸ¯ Learning rate: {config['learning_rate']}\")\n",
        "print(f\"   ğŸ”„ Total epochs: {config['epochs']} (freeze: {config['freeze_epochs']}, fine-tune: {config['finetune_epochs']})\")\n",
        "print(f\"   ğŸ‘¥ Workers: {config['num_workers']}\")\n",
        "print(f\"   âš–ï¸ Weight decay: {config['weight_decay']}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"\\nâœ… Configuration complete!\")\n",
        "print(\"ğŸ’¡ If you encounter memory issues, reduce batch_size to 16 or 8\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Data Preprocessing and DataLoader Setup\n",
        "\n",
        "### Data Augmentation Strategy\n",
        "\n",
        "**Why Augmentation is Critical for ResNet50:**\n",
        "- **Prevents Overfitting**: Deeper networks are more prone to overfitting\n",
        "- **Increases Effective Dataset Size**: More parameters need more data variations\n",
        "- **Improves Generalization**: Helps the model handle real-world variations\n",
        "- **Maximizes Transfer Learning**: Augmentation helps adaptation to new domain\n",
        "\n",
        "**Training vs. Validation Transforms:**\n",
        "- **Training**: Aggressive augmentation for robustness\n",
        "- **Validation/Test**: Minimal transforms for consistent evaluation\n",
        "\n",
        "### ImageNet Normalization\n",
        "Critical for pre-trained models - ResNet50 expects exact ImageNet statistics:\n",
        "- **Mean**: [0.485, 0.456, 0.406] for RGB channels\n",
        "- **Std**: [0.229, 0.224, 0.225] for RGB channels\n",
        "\n",
        "### Memory Considerations\n",
        "ResNet50 uses more memory than ResNet18:\n",
        "- **Batch Size**: May need reduction from 32 to 16 or 8\n",
        "- **Workers**: Monitor CPU usage during data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ”§ Creating data preprocessing pipeline...\")\n",
        "\n",
        "# Training transforms with augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation transforms (no augmentation)\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"   âœ“ Training transforms: 5 augmentations + ImageNet normalization\")\n",
        "print(\"   âœ“ Validation transforms: resize + ImageNet normalization only\")\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\nğŸ“¦ Loading Flowers102 dataset...\")\n",
        "try:\n",
        "    train_dataset = torchvision.datasets.Flowers102(\n",
        "        root='./data', split='train', transform=train_transforms, download=True)\n",
        "    val_dataset = torchvision.datasets.Flowers102(\n",
        "        root='./data', split='val', transform=val_transforms, download=True)\n",
        "    test_dataset = torchvision.datasets.Flowers102(\n",
        "        root='./data', split='test', transform=val_transforms, download=True)\n",
        "    \n",
        "    print(f\"   ğŸ‹ï¸ Training samples: {len(train_dataset):,}\")\n",
        "    print(f\"   ğŸ” Validation samples: {len(val_dataset):,}\")\n",
        "    print(f\"   ğŸ“ Test samples: {len(test_dataset):,}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   âŒ Error loading dataset: {e}\")\n",
        "    print(\"   ğŸ’¡ Make sure you have internet connection for first download\")\n",
        "\n",
        "# Create DataLoaders with memory monitoring\n",
        "print(\"\\nâš™ï¸ Setting up DataLoaders...\")\n",
        "try:\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
        "                             shuffle=True, num_workers=config['num_workers'], pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
        "                           shuffle=False, num_workers=config['num_workers'], pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], \n",
        "                            shuffle=False, num_workers=config['num_workers'], pin_memory=True)\n",
        "    \n",
        "    print(f\"   ğŸ“Š DataLoader batches: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test\")\n",
        "    print(\"   âœ… Data pipeline ready!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   âŒ Error creating DataLoaders: {e}\")\n",
        "    print(\"   ğŸ’¡ Try reducing batch_size or num_workers\")\n",
        "    print(\"   ğŸ’¡ Suggested fix: config['batch_size'] = 16\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: ResNet50 Model Setup and Architecture Analysis\n",
        "\n",
        "### ResNet50 vs ResNet18 Comparison\n",
        "\n",
        "| Feature | ResNet18 | ResNet50 | Impact |\n",
        "|---------|----------|----------|---------|\n",
        "| **Layers** | 18 | 50 | 2.8Ã— deeper |\n",
        "| **Parameters** | 11.7M | 25.6M | 2.2Ã— more |\n",
        "| **Model Size** | ~47MB | ~102MB | 2.2Ã— larger |\n",
        "| **Memory Usage** | ~2GB | ~3-4GB | 1.5-2Ã— more |\n",
        "| **Training Time** | 15-20 min | 25-35 min | 1.5-2Ã— slower |\n",
        "\n",
        "### Bottleneck Block Innovation\n",
        "ResNet50 uses bottleneck blocks instead of basic blocks:\n",
        "- **1Ã—1 Conv**: Reduces channels for efficiency\n",
        "- **3Ã—3 Conv**: Processes features with reduced channels\n",
        "- **1Ã—1 Conv**: Expands channels back to original size\n",
        "- **Skip Connection**: Enables deep network training\n",
        "\n",
        "### Transfer Learning Advantages\n",
        "ResNet50's depth provides:\n",
        "- **Richer Feature Hierarchy**: More complex pattern recognition\n",
        "- **Better Generalization**: Proven performance on diverse tasks\n",
        "- **Stable Training**: Residual connections prevent vanishing gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ—ï¸ Setting up ResNet50 model...\")\n",
        "\n",
        "# Load pre-trained ResNet50\n",
        "model = models.resnet50(pretrained=True)\n",
        "print(f\"   âœ“ Loaded pre-trained ResNet50\")\n",
        "print(f\"   ğŸ“Š Original final layer: {model.fc.in_features} â†’ 1000 classes\")\n",
        "\n",
        "# Modify final layer for Flowers102 (102 classes)\n",
        "num_classes = 102\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "print(f\"   ğŸ¯ Modified final layer: {model.fc.in_features} â†’ {num_classes} classes\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "print(f\"   ğŸš€ Model moved to {device}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"   ğŸ“ˆ Total parameters: {total_params:,}\")\n",
        "print(f\"   ğŸ¯ Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   ğŸ“Š Model size: {total_params * 4 / 1e6:.1f} MB (float32)\")\n",
        "\n",
        "# Compare with ResNet18\n",
        "resnet18_params = 11_689_512  # Known ResNet18 parameter count\n",
        "print(f\"\\nğŸ“Š ResNet50 vs ResNet18 comparison:\")\n",
        "print(f\"   ğŸ“ˆ Parameter ratio: {total_params / resnet18_params:.1f}Ã— more parameters\")\n",
        "print(f\"   ğŸ’¾ Memory ratio: {total_params / resnet18_params:.1f}Ã— more memory\")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "\n",
        "print(f\"\\nâš™ï¸ Training setup:\")\n",
        "print(f\"   ğŸ¯ Loss function: CrossEntropyLoss\")\n",
        "print(f\"   ğŸš€ Optimizer: AdamW (lr={config['learning_rate']}, weight_decay={config['weight_decay']})\")\n",
        "\n",
        "# Function to freeze/unfreeze model parameters\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Only train the classifier\n",
        "        for param in model.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "    else:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "print(\"âœ… ResNet50 model setup complete!\")\n",
        "print(\"ğŸ’¡ Ready for two-phase training: feature extraction â†’ fine-tuning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 5: Training and Evaluation Functions\n",
        "\n",
        "### Function Design for Deep Networks\n",
        "Our training functions are optimized for deeper networks like ResNet50:\n",
        "- **Memory Management**: Efficient GPU memory usage\n",
        "- **Progress Monitoring**: Real-time loss and accuracy tracking\n",
        "- **Error Handling**: Graceful handling of memory issues\n",
        "- **Performance Metrics**: Comprehensive evaluation\n",
        "\n",
        "### Training Strategy\n",
        "We use the same two-phase approach as ResNet18 for fair comparison:\n",
        "1. **Phase 1**: Feature extraction (frozen backbone)\n",
        "2. **Phase 2**: End-to-end fine-tuning (unfrozen network)\n",
        "\n",
        "### Memory Optimization\n",
        "The functions include automatic memory cleanup to handle ResNet50's higher memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch with memory optimization\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "    \n",
        "    for batch_idx, (data, targets) in enumerate(progress_bar):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "        \n",
        "        # Memory cleanup for ResNet50\n",
        "        del data, targets, outputs, loss\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    \"\"\"Evaluate model on validation set with memory optimization\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc=\"Evaluating\", leave=False)\n",
        "        \n",
        "        for batch_idx, (data, targets) in enumerate(progress_bar):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{val_loss/(batch_idx+1):.3f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "            \n",
        "            # Memory cleanup for ResNet50\n",
        "            del data, targets, outputs, loss\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "    \n",
        "    return val_loss / len(val_loader), 100. * correct / total\n",
        "\n",
        "print(\"âœ… Training and evaluation functions defined!\")\n",
        "print(\"ğŸ’¡ Functions include memory optimization for ResNet50\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 6: Phase 1 - Feature Extraction Training\n",
        "\n",
        "### Feature Extraction with ResNet50\n",
        "In Phase 1, we freeze the deeper ResNet50 backbone and train only the classifier:\n",
        "\n",
        "**Why This Works Well for ResNet50:**\n",
        "- **Pre-trained Features**: 50 layers of ImageNet features are very rich\n",
        "- **Computational Efficiency**: Only training ~100K parameters vs 25.6M\n",
        "- **Memory Efficiency**: Lower memory usage during backpropagation\n",
        "- **Stable Learning**: Avoids disturbing learned features initially\n",
        "\n",
        "**Expected Performance:**\n",
        "- **ResNet18**: ~75% accuracy after Phase 1\n",
        "- **ResNet50**: ~78% accuracy after Phase 1 (3% improvement)\n",
        "\n",
        "**Training Details:**\n",
        "- **Frozen Parameters**: 25.5M parameters (99.6% of network)\n",
        "- **Trainable Parameters**: ~100K parameters (final layer only)\n",
        "- **Duration**: 20 epochs (same as ResNet18 for comparison)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ¯ Phase 1: Feature Extraction Training (ResNet50)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Freeze backbone, only train classifier\n",
        "set_parameter_requires_grad(model, feature_extracting=True)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "frozen_params = total_params - trainable_params\n",
        "\n",
        "print(f\"   ğŸ”’ Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)\")\n",
        "print(f\"   ğŸ¯ Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
        "print(f\"   ğŸ“Š Training efficiency: {frozen_params/trainable_params:.0f}Ã— fewer parameters to train\")\n",
        "\n",
        "# Training tracking\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(f\"\\nğŸš€ Starting Phase 1 training ({config['freeze_epochs']} epochs)...\")\n",
        "print(\"ğŸ’¡ This may take longer than ResNet18 due to deeper network\")\n",
        "phase1_start = time.time()\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "try:\n",
        "    for epoch in range(config['freeze_epochs']):\n",
        "        epoch_start = time.time()\n",
        "        \n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "        \n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        # Record metrics\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        \n",
        "        print(f\"Epoch {epoch+1:2d}/{config['freeze_epochs']} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \"\n",
        "              f\"Time: {epoch_time:.1f}s\")\n",
        "        \n",
        "        # Memory monitoring\n",
        "        if torch.cuda.is_available():\n",
        "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
        "            if memory_used > 0.5:  # Show if using > 0.5GB\n",
        "                print(f\"           GPU Memory: {memory_used:.1f}GB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training error: {e}\")\n",
        "    print(\"ğŸ’¡ Try reducing batch_size in config if memory error\")\n",
        "\n",
        "phase1_time = time.time() - phase1_start\n",
        "\n",
        "print(f\"\\nğŸ“Š Phase 1 Results:\")\n",
        "print(f\"   â±ï¸  Training time: {phase1_time:.1f}s ({phase1_time/60:.1f}m)\")\n",
        "print(f\"   ğŸ¯ Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"   ğŸ“ˆ Final training accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "print(f\"   ğŸ“‰ Final validation loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "# ResNet18 comparison (expected values)\n",
        "resnet18_phase1_acc = 75.0  # Expected ResNet18 Phase 1 accuracy\n",
        "improvement = best_val_acc - resnet18_phase1_acc\n",
        "print(f\"\\nğŸ” Comparison with ResNet18:\")\n",
        "print(f\"   ğŸ“Š ResNet18 Phase 1: ~{resnet18_phase1_acc:.0f}%\")\n",
        "print(f\"   ğŸ“Š ResNet50 Phase 1: {best_val_acc:.2f}%\")\n",
        "print(f\"   ğŸš€ Improvement: {improvement:+.1f}% (deeper network advantage)\")\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(\"âœ… Phase 1 complete! Best model weights loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 7: Phase 2 - Fine-tuning Training\n",
        "\n",
        "### Fine-tuning ResNet50\n",
        "In Phase 2, we unfreeze all layers and train the entire ResNet50 network:\n",
        "\n",
        "**Why ResNet50 Fine-tuning is Powerful:**\n",
        "- **Deep Feature Adaptation**: 50 layers can adapt to flower-specific features\n",
        "- **Hierarchical Learning**: Low-level features adapt while high-level features fine-tune\n",
        "- **Superior Performance**: Deeper networks typically achieve better fine-tuning results\n",
        "- **Stable Training**: Residual connections enable stable deep network training\n",
        "\n",
        "**Expected Performance:**\n",
        "- **ResNet18**: ~85% accuracy after Phase 2\n",
        "- **ResNet50**: ~88% accuracy after Phase 2 (3% improvement)\n",
        "\n",
        "**Training Details:**\n",
        "- **Trainable Parameters**: All 25.6M parameters\n",
        "- **Memory Usage**: Significantly higher than Phase 1\n",
        "- **Training Time**: Longer per epoch due to full network backpropagation\n",
        "- **Learning Rate**: Same as ResNet18 (0.001) for fair comparison\n",
        "\n",
        "### Memory Management\n",
        "ResNet50 fine-tuning requires careful memory management:\n",
        "- **GPU Memory**: ~3-4GB required (vs ~2GB for ResNet18)\n",
        "- **Batch Size**: May need reduction if memory issues occur\n",
        "- **Gradient Accumulation**: Automatic cleanup helps prevent memory leaks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ”¥ Phase 2: Fine-tuning Training (ResNet50)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Unfreeze all layers for fine-tuning\n",
        "set_parameter_requires_grad(model, feature_extracting=False)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"   ğŸ”“ All parameters unfrozen\")\n",
        "print(f\"   ğŸ¯ Trainable parameters: {trainable_params:,} (100% of network)\")\n",
        "print(f\"   ğŸ“Š Full network training: {trainable_params/1e6:.1f}M parameters\")\n",
        "\n",
        "# Create new optimizer for fine-tuning\n",
        "optimizer_ft = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "\n",
        "print(f\"\\nğŸš€ Starting Phase 2 training ({config['finetune_epochs']} epochs)...\")\n",
        "print(\"ğŸ’¡ Fine-tuning will take longer than Phase 1 (full network backprop)\")\n",
        "print(\"ğŸ’¡ Memory usage will be higher - monitor for potential issues\")\n",
        "\n",
        "phase2_start = time.time()\n",
        "\n",
        "# Continue from Phase 1 metrics\n",
        "phase1_epochs = len(train_losses)\n",
        "current_best_val_acc = best_val_acc\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "try:\n",
        "    for epoch in range(config['finetune_epochs']):\n",
        "        epoch_start = time.time()\n",
        "        \n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer_ft, device)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "        \n",
        "        # Save best model\n",
        "        if val_acc > current_best_val_acc:\n",
        "            current_best_val_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        # Record metrics\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        \n",
        "        print(f\"Epoch {epoch+1:2d}/{config['finetune_epochs']} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \"\n",
        "              f\"Time: {epoch_time:.1f}s\")\n",
        "        \n",
        "        # Enhanced memory monitoring for fine-tuning\n",
        "        if torch.cuda.is_available():\n",
        "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
        "            memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
        "            print(f\"           GPU Memory: {memory_used:.1f}GB used, {memory_reserved:.1f}GB reserved\")\n",
        "            \n",
        "            # Warning if memory usage is high\n",
        "            if memory_used > 8:  # 8GB threshold\n",
        "                print(\"           âš ï¸  High memory usage - consider reducing batch size\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training error: {e}\")\n",
        "    print(\"ğŸ’¡ Common fixes for ResNet50:\")\n",
        "    print(\"   - Reduce batch_size to 16 or 8\")\n",
        "    print(\"   - Reduce num_workers to 1\")\n",
        "    print(\"   - Ensure sufficient GPU memory (>4GB recommended)\")\n",
        "\n",
        "phase2_time = time.time() - phase2_start\n",
        "total_time = phase1_time + phase2_time\n",
        "\n",
        "print(f\"\\nğŸ“Š Phase 2 Results:\")\n",
        "print(f\"   â±ï¸  Training time: {phase2_time:.1f}s ({phase2_time/60:.1f}m)\")\n",
        "print(f\"   ğŸ¯ Best validation accuracy: {current_best_val_acc:.2f}%\")\n",
        "print(f\"   ğŸ“ˆ Final training accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "\n",
        "print(f\"\\nğŸ‰ Complete ResNet50 Training Summary:\")\n",
        "print(f\"   â±ï¸  Total time: {total_time:.1f}s ({total_time/60:.1f}m)\")\n",
        "print(f\"   ğŸ“Š Phase 1 â†’ Phase 2: {val_accuracies[phase1_epochs-1]:.2f}% â†’ {current_best_val_acc:.2f}%\")\n",
        "print(f\"   ğŸš€ Fine-tuning gain: {current_best_val_acc - val_accuracies[phase1_epochs-1]:+.1f}%\")\n",
        "\n",
        "# Comprehensive comparison with ResNet18\n",
        "resnet18_final_acc = 85.0  # Expected ResNet18 final accuracy\n",
        "final_improvement = current_best_val_acc - resnet18_final_acc\n",
        "print(f\"\\nğŸ” Final ResNet50 vs ResNet18 Comparison:\")\n",
        "print(f\"   ğŸ“Š ResNet18 final: ~{resnet18_final_acc:.0f}%\")\n",
        "print(f\"   ğŸ“Š ResNet50 final: {current_best_val_acc:.2f}%\")\n",
        "print(f\"   ğŸš€ Depth advantage: {final_improvement:+.1f}%\")\n",
        "print(f\"   âš¡ Training time ratio: {total_time/1200:.1f}Ã— (ResNet18 ~20min baseline)\")\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(\"âœ… Phase 2 complete! Best ResNet50 model weights loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 8: Final Model Evaluation and Results Analysis\n",
        "\n",
        "### Test Set Evaluation\n",
        "Now we evaluate our trained ResNet50 model on the held-out test set:\n",
        "\n",
        "**Why Test Set Evaluation Matters:**\n",
        "- **Unbiased Performance**: Test set hasn't been seen during training\n",
        "- **Generalization Check**: Validates model's ability to handle new data\n",
        "- **Fair Comparison**: Consistent evaluation across all model architectures\n",
        "- **Real-world Simulation**: Mimics deployment performance\n",
        "\n",
        "### ResNet50 vs ResNet18 Final Comparison\n",
        "\n",
        "**Expected Results:**\n",
        "- **ResNet18 Test Accuracy**: ~83-85%\n",
        "- **ResNet50 Test Accuracy**: ~86-88%\n",
        "- **Improvement**: +3-5% from deeper architecture\n",
        "\n",
        "**Performance Analysis:**\n",
        "- **Computational Cost**: ResNet50 uses 2.2Ã— more parameters\n",
        "- **Training Time**: ResNet50 takes ~1.5-2Ã— longer to train\n",
        "- **Memory Usage**: ResNet50 requires ~1.5Ã— more GPU memory\n",
        "- **Accuracy Gain**: ResNet50 typically achieves 3-5% better accuracy\n",
        "\n",
        "### Key Insights from ResNet50 Training\n",
        "1. **Depth Benefits**: Deeper networks capture more complex features\n",
        "2. **Diminishing Returns**: Performance gains plateau with extreme depth\n",
        "3. **Memory Management**: Deeper networks require careful resource management\n",
        "4. **Transfer Learning**: Pre-trained features work well across architectures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ§ª Final Model Evaluation on Test Set\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"ğŸ” Evaluating trained ResNet50 on test set...\")\n",
        "test_start = time.time()\n",
        "\n",
        "try:\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    test_time = time.time() - test_start\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Test Set Results:\")\n",
        "    print(f\"   ğŸ¯ Test Accuracy: {test_acc:.2f}%\")\n",
        "    print(f\"   ğŸ“‰ Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"   â±ï¸  Evaluation time: {test_time:.1f}s\")\n",
        "    \n",
        "    # Performance analysis\n",
        "    print(f\"\\nğŸ“ˆ ResNet50 Performance Summary:\")\n",
        "    print(f\"   ğŸ¯ Final test accuracy: {test_acc:.2f}%\")\n",
        "    print(f\"   ğŸ“Š Best validation accuracy: {current_best_val_acc:.2f}%\")\n",
        "    print(f\"   ğŸ“‹ Generalization gap: {current_best_val_acc - test_acc:.2f}%\")\n",
        "    \n",
        "    # Compare with ResNet18\n",
        "    resnet18_test_acc = 84.0  # Expected ResNet18 test accuracy\n",
        "    improvement = test_acc - resnet18_test_acc\n",
        "    print(f\"\\nğŸ” Architecture Comparison:\")\n",
        "    print(f\"   ğŸ“Š ResNet18 (expected): ~{resnet18_test_acc:.0f}%\")\n",
        "    print(f\"   ğŸ“Š ResNet50 (actual): {test_acc:.2f}%\")\n",
        "    print(f\"   ğŸš€ Depth advantage: {improvement:+.1f}%\")\n",
        "    \n",
        "    # Efficiency analysis\n",
        "    print(f\"\\nâš¡ Efficiency Analysis:\")\n",
        "    print(f\"   ğŸ“ˆ Parameter count: {total_params:,} (2.2Ã— ResNet18)\")\n",
        "    print(f\"   â±ï¸  Training time: {total_time/60:.1f}m (~1.5Ã— ResNet18)\")\n",
        "    print(f\"   ğŸ¯ Accuracy per parameter: {test_acc/(total_params/1e6):.2f}% per M params\")\n",
        "    print(f\"   ğŸš€ Accuracy per minute: {test_acc/(total_time/60):.2f}% per minute\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Test evaluation error: {e}\")\n",
        "    print(\"ğŸ’¡ Ensure model and data are properly loaded\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ RESNET50 TRANSFER LEARNING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nğŸ“‹ Final Summary:\")\n",
        "print(f\"   ğŸ—ï¸  Architecture: ResNet50 (50 layers, 25.6M parameters)\")\n",
        "print(f\"   ğŸ“Š Dataset: Flowers102 (102 classes, 8,189 images)\")\n",
        "print(f\"   ğŸ¯ Final accuracy: {test_acc:.2f}%\")\n",
        "print(f\"   â±ï¸  Total training time: {total_time/60:.1f} minutes\")\n",
        "print(f\"   ğŸš€ Improvement over ResNet18: {improvement:+.1f}%\")\n",
        "\n",
        "print(f\"\\nğŸ“ Key Learnings:\")\n",
        "print(f\"   â€¢ Deeper networks (ResNet50) provide better feature extraction\")\n",
        "print(f\"   â€¢ Bottleneck blocks enable efficient deep network training\")\n",
        "print(f\"   â€¢ Transfer learning works exceptionally well with deeper models\")\n",
        "print(f\"   â€¢ Performance gains come at computational cost\")\n",
        "print(f\"   â€¢ Memory management becomes critical with deeper networks\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ Next Steps:\")\n",
        "print(f\"   â€¢ Lesson 5: EfficientNet-B0 (efficient architecture)\")\n",
        "print(f\"   â€¢ Lesson 6: EfficientNet-B3 (scaled efficient architecture)\")\n",
        "print(f\"   â€¢ Lesson 7: MobileNet-V2 (mobile-optimized architecture)\")\n",
        "print(f\"   â€¢ Compare all architectures for optimal model selection\")\n",
        "\n",
        "print(f\"\\nâ­ ResNet50 Transfer Learning Success!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
