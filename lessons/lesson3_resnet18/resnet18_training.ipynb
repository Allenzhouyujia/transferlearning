{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Lesson 3: ResNet18 Transfer Learning for Flower Classification\n",
        "\n",
        "## Overview\n",
        "Learn transfer learning with ResNet18 on the Flowers102 dataset. This lesson demonstrates how pre-trained models can be adapted for new classification tasks with minimal training time.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand ResNet18 architecture and residual connections\n",
        "- Implement transfer learning with pre-trained weights\n",
        "- Use progressive training strategy (freeze ‚Üí fine-tune)\n",
        "- Evaluate model performance and analyze results\n",
        "\n",
        "### Model Quick Facts\n",
        "- **Architecture**: ResNet18 (18 layers, 11.7M parameters)\n",
        "- **Pre-training**: ImageNet dataset (1.2M images, 1000 classes)\n",
        "- **Key Innovation**: Residual connections for deep network training\n",
        "- **Transfer Method**: Feature extraction + fine-tuning\n",
        "- **Expected Performance**: ~85%+ accuracy on Flowers102\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Environment Setup and Library Imports\n",
        "\n",
        "### Why This Step Matters\n",
        "Setting up the environment correctly is crucial for:\n",
        "- **Reproducibility**: Ensuring consistent results across different runs\n",
        "- **Performance**: Optimizing GPU usage and memory management\n",
        "- **Debugging**: Clean output without unnecessary warnings\n",
        "\n",
        "### Key Libraries Explained\n",
        "- **torch**: Core PyTorch library (tensors, automatic differentiation, neural networks)\n",
        "- **torchvision**: Computer vision utilities (datasets, transforms, pre-trained models)\n",
        "- **models**: Pre-trained model architectures (ResNet18, VGG, etc.)\n",
        "- **optim**: Optimization algorithms (SGD, Adam, AdamW)\n",
        "- **DataLoader**: Efficient batch processing and parallel data loading\n",
        "- **tqdm**: Progress bars for training loops\n",
        "- **matplotlib**: Data visualization and plotting\n",
        "- **sklearn**: Machine learning utilities (metrics, confusion matrix)\n",
        "\n",
        "### Configuration Settings\n",
        "We configure matplotlib for high-quality visualizations and set up proper warning filters for cleaner output during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n",
            "üì¶ PyTorch version: 2.2.2\n",
            "üñºÔ∏è Torchvision version: 0.17.2\n",
            "üî• CUDA available: False\n",
            "üçé MPS available: True\n"
          ]
        }
      ],
      "source": [
        "# Core PyTorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Computer vision utilities\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Machine learning utilities\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for high-quality plots\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.style.use('default')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
        "print(f\"üñºÔ∏è Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"üçé MPS available: {torch.backends.mps.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Device Detection and Configuration\n",
        "\n",
        "### Device Selection Strategy\n",
        "Transfer learning benefits significantly from GPU acceleration. Our device detection follows this priority:\n",
        "\n",
        "1. **CUDA GPU** (NVIDIA): Optimal for deep learning training\n",
        "   - Parallel processing with thousands of cores\n",
        "   - Large memory capacity for batch processing\n",
        "   - Highly optimized for matrix operations\n",
        "\n",
        "2. **MPS (Apple Silicon)**: Apple's Metal Performance Shaders\n",
        "   - Efficient on M1/M2 chips\n",
        "   - Unified memory architecture\n",
        "   - Good performance for development and medium-scale training\n",
        "\n",
        "3. **CPU**: Universal fallback\n",
        "   - Works on any system\n",
        "   - Slower but sufficient for learning purposes\n",
        "\n",
        "### Training Configuration\n",
        "We use standardized parameters for consistent comparison across all lessons:\n",
        "- **Batch Size**: 32 (balances memory usage and gradient quality)\n",
        "- **Learning Rate**: 0.001 (standard for AdamW optimizer)\n",
        "- **Epochs**: 50 total (20 frozen + 30 fine-tuning)\n",
        "- **Optimizer**: AdamW with weight decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Detecting optimal compute device...\n",
            "üçé Using Apple Silicon GPU (MPS)\n",
            "   Optimized for M1/M2 chips\n",
            "\n",
            "‚öôÔ∏è Setting up training configuration...\n",
            "   üì¶ Batch size: 32\n",
            "   üéØ Learning rate: 0.001\n",
            "   üîÑ Total epochs: 50 (freeze: 20, fine-tune: 30)\n",
            "   üë• Workers: 2\n",
            "   ‚öñÔ∏è Weight decay: 0.01\n",
            "\n",
            "‚úÖ Configuration complete!\n"
          ]
        }
      ],
      "source": [
        "# Device detection with fallback hierarchy\n",
        "print(\"üîç Detecting optimal compute device...\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"üöÄ Using NVIDIA GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"üçé Using Apple Silicon GPU (MPS)\")\n",
        "    print(\"   Optimized for M1/M2 chips\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"üíª Using CPU (consider GPU for faster training)\")\n",
        "\n",
        "# Set training configuration\n",
        "print(\"\\n‚öôÔ∏è Setting up training configuration...\")\n",
        "config = {\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 50,\n",
        "    'freeze_epochs': 20,\n",
        "    'finetune_epochs': 30,\n",
        "    'num_workers': 2,\n",
        "    'weight_decay': 0.01\n",
        "}\n",
        "\n",
        "print(f\"   üì¶ Batch size: {config['batch_size']}\")\n",
        "print(f\"   üéØ Learning rate: {config['learning_rate']}\")\n",
        "print(f\"   üîÑ Total epochs: {config['epochs']} (freeze: {config['freeze_epochs']}, fine-tune: {config['finetune_epochs']})\")\n",
        "print(f\"   üë• Workers: {config['num_workers']}\")\n",
        "print(f\"   ‚öñÔ∏è Weight decay: {config['weight_decay']}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"\\n‚úÖ Configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Data Preprocessing and DataLoader Setup\n",
        "\n",
        "### Data Augmentation Strategy\n",
        "\n",
        "**Why Augmentation is Critical:**\n",
        "- **Increases Effective Dataset Size**: Transforms create new variations of existing images\n",
        "- **Improves Generalization**: Model learns to handle variations in real-world data\n",
        "- **Reduces Overfitting**: Prevents memorization of specific image characteristics\n",
        "- **Handles Data Scarcity**: Particularly important for smaller datasets like Flowers102\n",
        "\n",
        "**Training vs. Validation Transforms:**\n",
        "- **Training**: Aggressive augmentation for maximum variety and robustness\n",
        "- **Validation/Test**: Minimal transforms for consistent, reproducible evaluation\n",
        "\n",
        "### ImageNet Normalization\n",
        "Pre-trained models require ImageNet statistics for optimal performance:\n",
        "- **Mean**: [0.485, 0.456, 0.406] for RGB channels\n",
        "- **Std**: [0.229, 0.224, 0.225] for RGB channels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating data preprocessing pipeline...\n",
            "   ‚úì Training transforms: 5 augmentations + normalization\n",
            "   ‚úì Validation transforms: resize + normalization only\n",
            "\n",
            "üì¶ Loading Flowers102 dataset...\n",
            "Downloading https://thor.robots.ox.ac.uk/flowers/102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 344862509/344862509 [00:19<00:00, 17583027.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/flowers/102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 502/502 [00:00<00:00, 177877.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/flowers/102/setid.mat to data/flowers-102/setid.mat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14989/14989 [00:00<00:00, 3870255.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üèãÔ∏è Training samples: 1,020\n",
            "   üîç Validation samples: 1,020\n",
            "   üìù Test samples: 6,149\n",
            "\n",
            "üìä DataLoader batches: 32 train, 32 val, 193 test\n",
            "‚úÖ Data pipeline ready!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"üîß Creating data preprocessing pipeline...\")\n",
        "\n",
        "# Training transforms with augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation transforms (no augmentation)\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"   ‚úì Training transforms: 5 augmentations + normalization\")\n",
        "print(\"   ‚úì Validation transforms: resize + normalization only\")\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\nüì¶ Loading Flowers102 dataset...\")\n",
        "train_dataset = torchvision.datasets.Flowers102(\n",
        "    root='./data', split='train', transform=train_transforms, download=True)\n",
        "val_dataset = torchvision.datasets.Flowers102(\n",
        "    root='./data', split='val', transform=val_transforms, download=True)\n",
        "test_dataset = torchvision.datasets.Flowers102(\n",
        "    root='./data', split='test', transform=val_transforms, download=True)\n",
        "\n",
        "print(f\"   üèãÔ∏è Training samples: {len(train_dataset):,}\")\n",
        "print(f\"   üîç Validation samples: {len(val_dataset):,}\")\n",
        "print(f\"   üìù Test samples: {len(test_dataset):,}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
        "                         shuffle=True, num_workers=config['num_workers'], pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
        "                       shuffle=False, num_workers=config['num_workers'], pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], \n",
        "                        shuffle=False, num_workers=config['num_workers'], pin_memory=True)\n",
        "\n",
        "print(f\"\\nüìä DataLoader batches: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test\")\n",
        "print(\"‚úÖ Data pipeline ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: ResNet18 Model Setup and Transfer Learning\n",
        "\n",
        "### ResNet18 Architecture Overview\n",
        "ResNet18 is a 18-layer deep convolutional neural network that introduced residual connections:\n",
        "\n",
        "**Key Features:**\n",
        "- **Residual Blocks**: Skip connections that help training very deep networks\n",
        "- **Batch Normalization**: Normalizes inputs to each layer for stable training\n",
        "- **ReLU Activation**: Non-linear activation function\n",
        "- **Global Average Pooling**: Reduces spatial dimensions before classification\n",
        "\n",
        "### Transfer Learning Strategy\n",
        "\n",
        "**Phase 1: Feature Extraction (Freeze Backbone)**\n",
        "- Keep pre-trained weights frozen\n",
        "- Only train the new classification head\n",
        "- Fast training, good for small datasets\n",
        "- Expected: ~75% accuracy after 20 epochs\n",
        "\n",
        "**Phase 2: Fine-tuning (Unfreeze All Layers)**\n",
        "- Gradually unfreeze all layers\n",
        "- Train entire network with lower learning rate\n",
        "- Better adaptation to target domain\n",
        "- Expected: ~85%+ accuracy after 30 more epochs\n",
        "\n",
        "### Model Modifications\n",
        "- Replace final layer: 1000 classes ‚Üí 102 classes\n",
        "- Keep all other layers with ImageNet weights\n",
        "- Use appropriate learning rates for each phase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è Setting up ResNet18 model...\n",
            "   ‚úì Loaded pre-trained ResNet18\n",
            "   üìä Original classes: 512 ‚Üí 1000\n",
            "   üéØ Modified final layer: 512 ‚Üí 102\n",
            "   üöÄ Model moved to mps\n",
            "   üìà Total parameters: 11,228,838\n",
            "   üéØ Trainable parameters: 11,228,838\n",
            "\n",
            "‚öôÔ∏è Training setup:\n",
            "   üéØ Loss function: CrossEntropyLoss\n",
            "   üöÄ Optimizer: AdamW (lr=0.001, weight_decay=0.01)\n",
            "‚úÖ Model setup complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"üèóÔ∏è Setting up ResNet18 model...\")\n",
        "\n",
        "# Load pre-trained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "print(f\"   ‚úì Loaded pre-trained ResNet18\")\n",
        "print(f\"   üìä Original classes: {model.fc.in_features} ‚Üí 1000\")\n",
        "\n",
        "# Modify final layer for Flowers102 (102 classes)\n",
        "num_classes = 102\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "print(f\"   üéØ Modified final layer: {model.fc.in_features} ‚Üí {num_classes}\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "print(f\"   üöÄ Model moved to {device}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"   üìà Total parameters: {total_params:,}\")\n",
        "print(f\"   üéØ Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Training setup:\")\n",
        "print(f\"   üéØ Loss function: CrossEntropyLoss\")\n",
        "print(f\"   üöÄ Optimizer: AdamW (lr={config['learning_rate']}, weight_decay={config['weight_decay']})\")\n",
        "\n",
        "# Function to freeze/unfreeze model parameters\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Only train the classifier\n",
        "        for param in model.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "    else:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "print(\"‚úÖ Model setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 5: Training Functions and Evaluation\n",
        "\n",
        "### Training Function Design\n",
        "Our training function implements:\n",
        "- **Batch Processing**: Efficient mini-batch gradient descent\n",
        "- **Progress Tracking**: Real-time loss and accuracy monitoring\n",
        "- **Memory Management**: Proper GPU memory cleanup\n",
        "- **Gradient Accumulation**: Stable training with consistent updates\n",
        "\n",
        "### Evaluation Metrics\n",
        "We track multiple metrics for comprehensive evaluation:\n",
        "- **Loss**: CrossEntropyLoss for optimization\n",
        "- **Accuracy**: Top-1 classification accuracy\n",
        "- **Progress**: Real-time training progress with tqdm\n",
        "- **Timing**: Training time per epoch for performance analysis\n",
        "\n",
        "### Two-Phase Training Process\n",
        "1. **Phase 1 (Epochs 1-20)**: Feature extraction with frozen backbone\n",
        "2. **Phase 2 (Epochs 21-50)**: Fine-tuning with unfrozen layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training and evaluation functions defined!\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "    \n",
        "    for batch_idx, (data, targets) in enumerate(progress_bar):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc=\"Evaluating\", leave=False)\n",
        "        \n",
        "        for batch_idx, (data, targets) in enumerate(progress_bar):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{val_loss/(batch_idx+1):.3f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    return val_loss / len(val_loader), 100. * correct / total\n",
        "\n",
        "print(\"‚úÖ Training and evaluation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 6: Phase 1 - Feature Extraction Training\n",
        "\n",
        "### Feature Extraction Strategy\n",
        "In the first phase, we freeze the pre-trained backbone and only train the classification head:\n",
        "\n",
        "**Why Feature Extraction First?**\n",
        "- **Preserves Pre-trained Features**: Keeps valuable ImageNet features intact\n",
        "- **Faster Training**: Only ~50K parameters to train vs. 11.7M total\n",
        "- **Stable Learning**: Prevents catastrophic forgetting of pre-trained weights\n",
        "- **Good Baseline**: Achieves decent performance quickly\n",
        "\n",
        "**Training Details:**\n",
        "- **Frozen Layers**: All convolutional layers and batch normalization\n",
        "- **Trainable Layers**: Only the final classification layer (fc)\n",
        "- **Learning Rate**: 0.001 (standard for new layers)\n",
        "- **Duration**: 20 epochs (sufficient for classifier convergence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Phase 1: Feature Extraction Training\n",
            "==================================================\n",
            "   üîí Frozen parameters: 11,176,512\n",
            "   üéØ Trainable parameters: 52,326\n",
            "\n",
            "üöÄ Starting Phase 1 training (20 epochs)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1/20 | Train Loss: 4.7374 | Train Acc: 3.14% | Val Loss: 4.0240 | Val Acc: 13.04% | Time: 34.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2/20 | Train Loss: 3.6848 | Train Acc: 24.51% | Val Loss: 3.2799 | Val Acc: 40.00% | Time: 34.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3/20 | Train Loss: 2.9507 | Train Acc: 50.10% | Val Loss: 2.7430 | Val Acc: 52.06% | Time: 36.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4/20 | Train Loss: 2.3572 | Train Acc: 64.71% | Val Loss: 2.3180 | Val Acc: 57.16% | Time: 34.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5/20 | Train Loss: 1.9245 | Train Acc: 76.76% | Val Loss: 1.9695 | Val Acc: 65.10% | Time: 35.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6/20 | Train Loss: 1.6206 | Train Acc: 80.59% | Val Loss: 1.7696 | Val Acc: 67.25% | Time: 34.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7/20 | Train Loss: 1.3733 | Train Acc: 81.86% | Val Loss: 1.5568 | Val Acc: 71.37% | Time: 34.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8/20 | Train Loss: 1.1968 | Train Acc: 84.71% | Val Loss: 1.4594 | Val Acc: 71.47% | Time: 34.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9/20 | Train Loss: 1.0417 | Train Acc: 87.65% | Val Loss: 1.3446 | Val Acc: 73.92% | Time: 35.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 | Train Loss: 0.9244 | Train Acc: 87.94% | Val Loss: 1.2627 | Val Acc: 74.12% | Time: 35.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 | Train Loss: 0.8165 | Train Acc: 91.18% | Val Loss: 1.2062 | Val Acc: 75.10% | Time: 34.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20 | Train Loss: 0.7405 | Train Acc: 91.76% | Val Loss: 1.1422 | Val Acc: 75.69% | Time: 34.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/20 | Train Loss: 0.6826 | Train Acc: 93.24% | Val Loss: 1.0926 | Val Acc: 76.47% | Time: 34.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/20 | Train Loss: 0.6244 | Train Acc: 93.14% | Val Loss: 1.0665 | Val Acc: 77.35% | Time: 34.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 | Train Loss: 0.5694 | Train Acc: 94.80% | Val Loss: 1.0419 | Val Acc: 77.55% | Time: 34.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/20 | Train Loss: 0.5238 | Train Acc: 94.41% | Val Loss: 1.0391 | Val Acc: 76.27% | Time: 33.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/20 | Train Loss: 0.4786 | Train Acc: 95.88% | Val Loss: 1.0039 | Val Acc: 77.84% | Time: 34.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/20 | Train Loss: 0.4695 | Train Acc: 94.22% | Val Loss: 0.9707 | Val Acc: 77.65% | Time: 33.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/20 | Train Loss: 0.4249 | Train Acc: 95.59% | Val Loss: 0.9550 | Val Acc: 77.75% | Time: 34.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/20 | Train Loss: 0.3979 | Train Acc: 96.67% | Val Loss: 0.9296 | Val Acc: 77.65% | Time: 35.7s\n",
            "\n",
            "üìä Phase 1 Results:\n",
            "   ‚è±Ô∏è  Training time: 693.8s (11.6m)\n",
            "   üéØ Best validation accuracy: 77.84%\n",
            "   üìà Final training accuracy: 96.67%\n",
            "   üìâ Final validation loss: 0.9296\n",
            "‚úÖ Phase 1 complete! Best model weights loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(\"üéØ Phase 1: Feature Extraction Training\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Freeze backbone, only train classifier\n",
        "set_parameter_requires_grad(model, feature_extracting=True)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"   üîí Frozen parameters: {total_params - trainable_params:,}\")\n",
        "print(f\"   üéØ Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Training tracking\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(f\"\\nüöÄ Starting Phase 1 training ({config['freeze_epochs']} epochs)...\")\n",
        "phase1_start = time.time()\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "for epoch in range(config['freeze_epochs']):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    # Record metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    print(f\"Epoch {epoch+1:2d}/{config['freeze_epochs']} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \"\n",
        "          f\"Time: {epoch_time:.1f}s\")\n",
        "\n",
        "phase1_time = time.time() - phase1_start\n",
        "\n",
        "print(f\"\\nüìä Phase 1 Results:\")\n",
        "print(f\"   ‚è±Ô∏è  Training time: {phase1_time:.1f}s ({phase1_time/60:.1f}m)\")\n",
        "print(f\"   üéØ Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"   üìà Final training accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "print(f\"   üìâ Final validation loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(\"‚úÖ Phase 1 complete! Best model weights loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 7: Phase 2 - Fine-tuning Training\n",
        "\n",
        "### Fine-tuning Strategy\n",
        "In the second phase, we unfreeze all layers and train the entire network:\n",
        "\n",
        "**Why Fine-tuning After Feature Extraction?**\n",
        "- **Better Adaptation**: Allows all layers to adapt to the new flower domain\n",
        "- **Higher Performance**: Typically achieves 5-15% better accuracy\n",
        "- **Stable Foundation**: Phase 1 provides a good starting point\n",
        "- **Controlled Learning**: Lower learning rate prevents catastrophic forgetting\n",
        "\n",
        "**Training Details:**\n",
        "- **Unfrozen Layers**: All 11.7M parameters are now trainable\n",
        "- **Learning Rate**: 0.001 (same as Phase 1, but now for entire network)\n",
        "- **Duration**: 30 epochs (longer for full network convergence)\n",
        "- **Expected Improvement**: ~75% ‚Üí ~85%+ validation accuracy\n",
        "\n",
        "### Learning Rate Considerations\n",
        "- **Same LR**: We keep the same learning rate since the model is already partially trained\n",
        "- **Lower LR Option**: Could use 0.0001 for more conservative fine-tuning\n",
        "- **Scheduler**: Could add learning rate scheduling for better convergence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî• Phase 2: Fine-tuning Training\n",
            "==================================================\n",
            "   üîì All parameters unfrozen\n",
            "   üéØ Trainable parameters: 11,228,838\n",
            "\n",
            "üöÄ Starting Phase 2 training (30 epochs)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1/30 | Train Loss: 1.6831 | Train Acc: 53.82% | Val Loss: 3.2129 | Val Acc: 38.14% | Time: 44.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2/30 | Train Loss: 0.8670 | Train Acc: 75.39% | Val Loss: 1.4263 | Val Acc: 65.20% | Time: 38.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3/30 | Train Loss: 0.4729 | Train Acc: 86.57% | Val Loss: 1.8036 | Val Acc: 55.20% | Time: 39.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4/30 | Train Loss: 0.4005 | Train Acc: 89.02% | Val Loss: 1.0879 | Val Acc: 71.27% | Time: 37.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5/30 | Train Loss: 0.2731 | Train Acc: 92.94% | Val Loss: 1.2216 | Val Acc: 70.39% | Time: 38.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6/30 | Train Loss: 0.1838 | Train Acc: 94.61% | Val Loss: 0.9953 | Val Acc: 75.39% | Time: 39.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7/30 | Train Loss: 0.1191 | Train Acc: 97.16% | Val Loss: 0.9962 | Val Acc: 76.27% | Time: 39.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8/30 | Train Loss: 0.1079 | Train Acc: 98.14% | Val Loss: 0.9595 | Val Acc: 75.78% | Time: 37.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9/30 | Train Loss: 0.0991 | Train Acc: 97.75% | Val Loss: 1.4122 | Val Acc: 66.96% | Time: 38.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/30 | Train Loss: 0.1452 | Train Acc: 95.20% | Val Loss: 1.4275 | Val Acc: 68.04% | Time: 36.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/30 | Train Loss: 0.2046 | Train Acc: 94.22% | Val Loss: 1.2325 | Val Acc: 69.61% | Time: 35.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/30 | Train Loss: 0.2130 | Train Acc: 93.82% | Val Loss: 1.1445 | Val Acc: 69.90% | Time: 35.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/30 | Train Loss: 0.2279 | Train Acc: 94.02% | Val Loss: 1.5748 | Val Acc: 64.02% | Time: 36.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/30 | Train Loss: 0.2489 | Train Acc: 92.25% | Val Loss: 1.3972 | Val Acc: 67.25% | Time: 37.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/30 | Train Loss: 0.1713 | Train Acc: 94.90% | Val Loss: 1.2595 | Val Acc: 70.69% | Time: 38.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/30 | Train Loss: 0.0981 | Train Acc: 97.25% | Val Loss: 1.1442 | Val Acc: 74.31% | Time: 36.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/30 | Train Loss: 0.0934 | Train Acc: 97.75% | Val Loss: 1.0902 | Val Acc: 74.31% | Time: 37.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/30 | Train Loss: 0.1076 | Train Acc: 97.35% | Val Loss: 1.3564 | Val Acc: 70.10% | Time: 36.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/30 | Train Loss: 0.0988 | Train Acc: 97.25% | Val Loss: 1.0851 | Val Acc: 75.29% | Time: 37.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/30 | Train Loss: 0.1062 | Train Acc: 97.25% | Val Loss: 1.1164 | Val Acc: 74.12% | Time: 38.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/30 | Train Loss: 0.0996 | Train Acc: 96.86% | Val Loss: 1.1690 | Val Acc: 73.43% | Time: 37.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/30 | Train Loss: 0.0796 | Train Acc: 98.04% | Val Loss: 1.1268 | Val Acc: 74.22% | Time: 38.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/30 | Train Loss: 0.0870 | Train Acc: 97.55% | Val Loss: 0.8630 | Val Acc: 79.90% | Time: 35.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/30 | Train Loss: 0.0557 | Train Acc: 98.73% | Val Loss: 0.9525 | Val Acc: 76.86% | Time: 38.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/30 | Train Loss: 0.0703 | Train Acc: 97.94% | Val Loss: 1.1954 | Val Acc: 72.65% | Time: 38.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/30 | Train Loss: 0.0909 | Train Acc: 96.96% | Val Loss: 1.3484 | Val Acc: 69.22% | Time: 38.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/30 | Train Loss: 0.0653 | Train Acc: 97.94% | Val Loss: 1.1309 | Val Acc: 75.00% | Time: 39.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/30 | Train Loss: 0.0440 | Train Acc: 98.73% | Val Loss: 0.8884 | Val Acc: 78.24% | Time: 39.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/30 | Train Loss: 0.0551 | Train Acc: 98.63% | Val Loss: 0.8927 | Val Acc: 79.80% | Time: 38.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/30 | Train Loss: 0.0542 | Train Acc: 98.53% | Val Loss: 1.1363 | Val Acc: 75.39% | Time: 78.9s\n",
            "\n",
            "üìä Phase 2 Results:\n",
            "   ‚è±Ô∏è  Training time: 1182.5s (19.7m)\n",
            "   üéØ Best validation accuracy: 79.90%\n",
            "\n",
            "üéâ Complete Training Summary:\n",
            "   ‚è±Ô∏è  Total time: 1876.3s (31.3m)\n",
            "   üìä Phase 1 ‚Üí Phase 2 improvement: 77.65% ‚Üí 79.90%\n",
            "‚úÖ Phase 2 complete! Best model weights loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(\"üî• Phase 2: Fine-tuning Training\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Unfreeze all layers\n",
        "set_parameter_requires_grad(model, feature_extracting=False)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"   üîì All parameters unfrozen\")\n",
        "print(f\"   üéØ Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Create new optimizer for fine-tuning\n",
        "optimizer_ft = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "\n",
        "print(f\"\\nüöÄ Starting Phase 2 training ({config['finetune_epochs']} epochs)...\")\n",
        "phase2_start = time.time()\n",
        "\n",
        "# Continue from Phase 1 metrics\n",
        "phase1_epochs = len(train_losses)\n",
        "best_val_acc = max(val_accuracies)\n",
        "\n",
        "for epoch in range(config['finetune_epochs']):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer_ft, device)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    # Record metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    print(f\"Epoch {epoch+1:2d}/{config['finetune_epochs']} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \"\n",
        "          f\"Time: {epoch_time:.1f}s\")\n",
        "\n",
        "phase2_time = time.time() - phase2_start\n",
        "total_time = phase1_time + phase2_time\n",
        "\n",
        "print(f\"\\nüìä Phase 2 Results:\")\n",
        "print(f\"   ‚è±Ô∏è  Training time: {phase2_time:.1f}s ({phase2_time/60:.1f}m)\")\n",
        "print(f\"   üéØ Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nüéâ Complete Training Summary:\")\n",
        "print(f\"   ‚è±Ô∏è  Total time: {total_time:.1f}s ({total_time/60:.1f}m)\")\n",
        "print(f\"   üìä Phase 1 ‚Üí Phase 2 improvement: {val_accuracies[phase1_epochs-1]:.2f}% ‚Üí {best_val_acc:.2f}%\")\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(\"‚úÖ Phase 2 complete! Best model weights loaded.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl_course_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
