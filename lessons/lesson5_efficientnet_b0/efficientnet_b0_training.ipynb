{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Lesson 5: EfficientNet-B0 Transfer Learning for Flower Classification\n",
        "\n",
        "## Overview\n",
        "Learn transfer learning with EfficientNet-B0, a highly efficient neural architecture that achieves excellent performance with fewer parameters than traditional CNNs. This lesson demonstrates how modern efficient architectures revolutionize the accuracy-vs-efficiency trade-off.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand EfficientNet architecture and compound scaling\n",
        "- Implement transfer learning with efficient architectures  \n",
        "- Compare efficiency metrics: accuracy vs parameters vs speed\n",
        "- Analyze the trade-offs between model complexity and performance\n",
        "\n",
        "### Model Quick Facts\n",
        "- **Architecture**: EfficientNet-B0 (efficient CNN with 5.3M parameters)\n",
        "- **Pre-training**: ImageNet dataset (1.2M images, 1000 classes)\n",
        "- **Key Innovation**: Compound scaling + MBConv blocks + SE attention\n",
        "- **Transfer Method**: Feature extraction + fine-tuning\n",
        "- **Expected Performance**: ~90%+ accuracy on Flowers102 (best yet!)\n",
        "- **Efficiency**: 2.2√ó fewer parameters than ResNet18, ~5% better accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Environment Setup and Library Imports\n",
        "\n",
        "### Why This Step Matters\n",
        "Setting up the environment correctly is crucial for:\n",
        "- **Reproducibility**: Ensuring consistent results across different runs\n",
        "- **Performance**: Optimizing GPU usage and memory management  \n",
        "- **Compatibility**: EfficientNet requires specific PyTorch versions\n",
        "\n",
        "### Key Libraries Explained\n",
        "- **torch**: Core PyTorch library (tensors, automatic differentiation, neural networks)\n",
        "- **torchvision**: Computer vision utilities (datasets, transforms, pre-trained models)\n",
        "- **models**: Pre-trained model architectures (EfficientNet, ResNet, etc.)\n",
        "- **optim**: Optimization algorithms (SGD, Adam, AdamW)\n",
        "- **DataLoader**: Efficient batch processing and parallel data loading\n",
        "- **tqdm**: Progress bars for training loops\n",
        "- **matplotlib**: Data visualization and plotting\n",
        "- **sklearn**: Machine learning utilities (metrics, confusion matrix)\n",
        "\n",
        "### EfficientNet Compatibility\n",
        "EfficientNet requires PyTorch 1.6+ for proper Swish/SiLU activation support.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core PyTorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Computer vision utilities\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Machine learning utilities\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for high-quality plots\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.style.use('default')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
        "print(f\"üñºÔ∏è Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"üçé MPS available: {torch.backends.mps.is_available()}\")\n",
        "\n",
        "# Check EfficientNet availability\n",
        "try:\n",
        "    test_model = models.efficientnet_b0(pretrained=False)\n",
        "    print(\"‚úÖ EfficientNet-B0 available!\")\n",
        "    del test_model  # Clean up\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå EfficientNet not available: {e}\")\n",
        "    print(\"üí° Please update PyTorch/torchvision to latest version\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Data Loading and Preprocessing\n",
        "\n",
        "### Dataset Overview\n",
        "- **Flowers102**: 102 flower categories, 8,189 images\n",
        "- **Split**: 1,020 training, 1,020 validation, 6,149 test images\n",
        "- **Image Size**: Variable, we'll resize to 224√ó224 for EfficientNet-B0\n",
        "\n",
        "### Data Preprocessing Pipeline\n",
        "The preprocessing pipeline is crucial for EfficientNet's performance:\n",
        "\n",
        "**Training Transforms:**\n",
        "- **Resize**: 256√ó256 (slightly larger than target)\n",
        "- **RandomCrop**: 224√ó224 (random position for data augmentation)\n",
        "- **RandomHorizontalFlip**: 50% probability (geometric augmentation)\n",
        "- **ImageNet Normalization**: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "\n",
        "**Validation Transforms:**\n",
        "- **Resize**: 224√ó224 (direct resize, no augmentation)\n",
        "- **ImageNet Normalization**: Same as training\n",
        "\n",
        "### Why ImageNet Normalization?\n",
        "The pre-trained EfficientNet-B0 model was trained on ImageNet with specific normalization values. Using the same normalization ensures the input data distribution matches what the model expects during pre-training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "print(\"üìÅ Loading Flowers102 dataset...\")\n",
        "try:\n",
        "    train_dataset = torchvision.datasets.Flowers102(\n",
        "        root='./data', \n",
        "        split='train',\n",
        "        transform=train_transforms,\n",
        "        download=True\n",
        "    )\n",
        "    \n",
        "    val_dataset = torchvision.datasets.Flowers102(\n",
        "        root='./data', \n",
        "        split='val',\n",
        "        transform=val_transforms,\n",
        "        download=False\n",
        "    )\n",
        "    \n",
        "    test_dataset = torchvision.datasets.Flowers102(\n",
        "        root='./data', \n",
        "        split='test',\n",
        "        transform=val_transforms,\n",
        "        download=False\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"üìä Training images: {len(train_dataset)}\")\n",
        "    print(f\"üìä Validation images: {len(val_dataset)}\")\n",
        "    print(f\"üìä Test images: {len(test_dataset)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {e}\")\n",
        "    print(\"üí° Make sure you have internet connection for first download\")\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "print(f\"üîÑ Data loaders created with batch size: {BATCH_SIZE}\")\n",
        "print(f\"üì¶ Training batches: {len(train_loader)}\")\n",
        "print(f\"üì¶ Validation batches: {len(val_loader)}\")\n",
        "print(f\"üì¶ Test batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Model Architecture and Initialization\n",
        "\n",
        "### EfficientNet-B0 Architecture\n",
        "EfficientNet-B0 is the baseline model in the EfficientNet family, designed for optimal efficiency:\n",
        "\n",
        "**Key Features:**\n",
        "- **Parameters**: 5.3M (2.2√ó fewer than ResNet18)\n",
        "- **FLOPs**: 0.39G (4.6√ó fewer than ResNet18)\n",
        "- **ImageNet Accuracy**: 77.3% (higher than ResNet18's 69.8%)\n",
        "- **Architecture**: MBConv blocks with Squeeze-and-Excitation (SE) attention\n",
        "\n",
        "### Model Components:\n",
        "1. **Stem**: Initial 3√ó3 convolution with stride 2\n",
        "2. **MBConv Blocks**: Mobile inverted bottleneck convolutions (7 stages)\n",
        "3. **SE Attention**: Channel attention mechanism in each block\n",
        "4. **Head**: Global average pooling + fully connected layer\n",
        "\n",
        "### Transfer Learning Approach:\n",
        "- **Pre-trained Weights**: ImageNet (1000 classes) ‚Üí Flowers102 (102 classes)\n",
        "- **Classifier Replacement**: Replace final FC layer (1000 ‚Üí 102 neurons)\n",
        "- **Two-Phase Training**: Feature extraction ‚Üí fine-tuning\n",
        "\n",
        "### Why EfficientNet-B0 for Transfer Learning?\n",
        "- **Efficiency**: Faster training and inference\n",
        "- **Generalization**: Better feature extraction despite fewer parameters\n",
        "- **Scalability**: Easy to scale up to B1-B7 if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f\"üîß Using device: {device}\")\n",
        "\n",
        "# Initialize EfficientNet-B0 model\n",
        "print(\"üèóÔ∏è Initializing EfficientNet-B0 model...\")\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "# Print model information\n",
        "print(f\"üìä Model loaded with pre-trained ImageNet weights\")\n",
        "print(f\"üî¢ Original classifier input features: {model.classifier[1].in_features}\")\n",
        "print(f\"üî¢ Original classifier output classes: {model.classifier[1].out_features}\")\n",
        "\n",
        "# Replace classifier for 102 flower classes\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(model.classifier[1].in_features, 102)\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"\\nüìà Model Architecture Summary:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Model size: ~{total_params * 4 / 1024 / 1024:.1f}MB\")\n",
        "\n",
        "# Display model structure\n",
        "print(f\"\\nüèóÔ∏è Model Structure:\")\n",
        "print(f\"   Features: {sum(p.numel() for p in model.features.parameters()):,} parameters\")\n",
        "print(f\"   Classifier: {sum(p.numel() for p in model.classifier.parameters()):,} parameters\")\n",
        "\n",
        "# Print expected efficiency\n",
        "print(f\"\\n‚ö° Efficiency Comparison:\")\n",
        "print(f\"   ResNet18: ~11.7M params, ~85% expected accuracy\")\n",
        "print(f\"   ResNet50: ~25.6M params, ~88% expected accuracy\")\n",
        "print(f\"   EfficientNet-B0: ~5.3M params, ~90% expected accuracy\")\n",
        "print(f\"   Efficiency ratio: {90/5.3:.1f}% per M params (EfficientNet-B0)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Training Configuration\n",
        "\n",
        "### Two-Phase Training Strategy\n",
        "To ensure fair comparison across all models, we use the same training strategy:\n",
        "\n",
        "**Phase 1: Feature Extraction (Epochs 1-20)**\n",
        "- **Freeze**: All feature layers (backbone)\n",
        "- **Train**: Only classifier layer\n",
        "- **Purpose**: Adapt classifier to new classes while preserving pre-trained features\n",
        "- **Expected**: ~80% accuracy (better than ResNet due to superior features)\n",
        "\n",
        "**Phase 2: Fine-tuning (Epochs 21-50)**\n",
        "- **Unfreeze**: All layers\n",
        "- **Train**: Entire model with lower learning rate\n",
        "- **Purpose**: Fine-tune features for flower classification\n",
        "- **Expected**: ~90%+ accuracy (best performance yet)\n",
        "\n",
        "### Training Parameters\n",
        "- **Optimizer**: AdamW (weight decay for regularization)\n",
        "- **Learning Rate**: 0.001 (standard for transfer learning)\n",
        "- **Batch Size**: 32 (balanced for efficiency and stability)\n",
        "- **Total Epochs**: 50 (20 feature extraction + 30 fine-tuning)\n",
        "- **Loss Function**: CrossEntropyLoss (multi-class classification)\n",
        "\n",
        "### Why This Configuration?\n",
        "- **AdamW**: Better convergence than SGD for efficient architectures\n",
        "- **Learning Rate**: Pre-trained models need lower LR to avoid catastrophic forgetting\n",
        "- **Two-Phase**: Prevents overfitting while maximizing transfer learning benefits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# Phase 1: Feature Extraction (freeze backbone)\n",
        "print(\"üîí Phase 1: Feature Extraction Setup\")\n",
        "print(\"   Freezing feature layers...\")\n",
        "\n",
        "# Freeze all feature layers\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Keep classifier trainable\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Count trainable parameters for Phase 1\n",
        "trainable_params_phase1 = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"   Trainable parameters in Phase 1: {trainable_params_phase1:,}\")\n",
        "\n",
        "# Loss function and optimizer for Phase 1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_phase1 = optim.AdamW(model.classifier.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "print(f\"‚úÖ Phase 1 configuration complete\")\n",
        "print(f\"   Loss function: CrossEntropyLoss\")\n",
        "print(f\"   Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
        "print(f\"   Training epochs: 1-20\")\n",
        "\n",
        "# Prepare for Phase 2 setup (will be used later)\n",
        "print(f\"\\nüîì Phase 2: Fine-tuning Setup (will be activated at epoch 21)\")\n",
        "print(f\"   Will unfreeze all layers\")\n",
        "print(f\"   Will train entire model with same learning rate\")\n",
        "print(f\"   Training epochs: 21-50\")\n",
        "\n",
        "# Training tracking variables\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "print(f\"\\nüìä Training tracking initialized\")\n",
        "print(f\"   Metrics: loss, accuracy, training time\")\n",
        "print(f\"   Total epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   Expected training time: ~10-15 minutes\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 5: Training and Evaluation Functions\n",
        "\n",
        "### Training Function Features\n",
        "Our training function includes:\n",
        "- **Progress Tracking**: Real-time loss and accuracy monitoring\n",
        "- **Batch Processing**: Efficient mini-batch gradient descent\n",
        "- **Device Management**: Automatic GPU/CPU tensor placement\n",
        "- **Memory Optimization**: Gradient accumulation and clearing\n",
        "\n",
        "### Evaluation Function Features\n",
        "- **No Gradient Computation**: Faster inference with torch.no_grad()\n",
        "- **Batch Processing**: Efficient validation on entire dataset\n",
        "- **Accuracy Calculation**: Correct predictions / total predictions\n",
        "- **Memory Efficient**: Processes data in batches to handle large datasets\n",
        "\n",
        "### Key Implementation Details\n",
        "- **Mixed Precision**: Can be enabled for faster training on modern GPUs\n",
        "- **Gradient Clipping**: Prevents exploding gradients (optional)\n",
        "- **Learning Rate Scheduling**: Could be added for better convergence\n",
        "- **Early Stopping**: Could be implemented based on validation loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Progress bar\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "    \n",
        "    for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "        \n",
        "        for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print(\"‚úÖ Training and evaluation functions defined\")\n",
        "print(\"   train_epoch(): Trains model for one epoch with progress tracking\")\n",
        "print(\"   evaluate(): Evaluates model on validation set with accuracy calculation\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 6: Main Training Loop\n",
        "\n",
        "### Training Process Overview\n",
        "The training loop implements our two-phase strategy:\n",
        "\n",
        "**Phase 1 (Epochs 1-20): Feature Extraction**\n",
        "- Only classifier is trainable (~104K parameters)\n",
        "- Faster training due to fewer parameters\n",
        "- Expected rapid initial improvement\n",
        "- Should reach ~80% accuracy\n",
        "\n",
        "**Phase 2 (Epochs 21-50): Fine-tuning**\n",
        "- All parameters trainable (~5.3M parameters)\n",
        "- Slower training but better final performance\n",
        "- Fine-tunes pre-trained features for flowers\n",
        "- Should reach ~90%+ accuracy\n",
        "\n",
        "### Expected Training Timeline\n",
        "- **Phase 1**: Fast convergence, 30-60 seconds per epoch\n",
        "- **Phase 2**: Slower convergence, 1-2 minutes per epoch\n",
        "- **Total Time**: ~10-15 minutes (faster than ResNet!)\n",
        "\n",
        "### Monitoring Progress\n",
        "- **Training Loss**: Should decrease steadily\n",
        "- **Validation Loss**: Should decrease without overfitting\n",
        "- **Training Accuracy**: Should increase to 95%+\n",
        "- **Validation Accuracy**: Should reach 90%+ (best so far!)\n",
        "\n",
        "### Phase Transition\n",
        "At epoch 21, we'll automatically:\n",
        "- Unfreeze all layers\n",
        "- Create new optimizer for all parameters\n",
        "- Continue training with same learning rate\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 6: Main Training Loop\n",
        "\n",
        "### Training Process Overview\n",
        "The training loop implements our two-phase strategy:\n",
        "\n",
        "**Phase 1 (Epochs 1-20): Feature Extraction**\n",
        "- Only classifier is trainable (~104K parameters)\n",
        "- Faster training due to fewer parameters\n",
        "- Expected rapid initial improvement\n",
        "- Should reach ~80% accuracy\n",
        "\n",
        "**Phase 2 (Epochs 21-50): Fine-tuning**\n",
        "- All parameters trainable (~5.3M parameters)\n",
        "- Slower training but better final performance\n",
        "- Fine-tunes pre-trained features for flowers\n",
        "- Should reach ~90%+ accuracy\n",
        "\n",
        "### Expected Training Timeline\n",
        "- **Phase 1**: Fast convergence, 30-60 seconds per epoch\n",
        "- **Phase 2**: Slower convergence, 1-2 minutes per epoch\n",
        "- **Total Time**: ~10-15 minutes (faster than ResNet!)\n",
        "\n",
        "### Monitoring Progress\n",
        "- **Training Loss**: Should decrease steadily\n",
        "- **Validation Loss**: Should decrease without overfitting\n",
        "- **Training Accuracy**: Should increase to 95%+\n",
        "- **Validation Accuracy**: Should reach 90%+ (best so far!)\n",
        "\n",
        "### Phase Transition\n",
        "At epoch 21, we'll automatically:\n",
        "- Unfreeze all layers\n",
        "- Create new optimizer for all parameters\n",
        "- Continue training with same learning rate\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Model Architecture and Initialization\n",
        "\n",
        "### EfficientNet-B0 Architecture\n",
        "EfficientNet-B0 is the baseline model in the EfficientNet family, designed for optimal efficiency:\n",
        "\n",
        "**Key Features:**\n",
        "- **Parameters**: 5.3M (2.2√ó fewer than ResNet18)\n",
        "- **FLOPs**: 0.39G (4.6√ó fewer than ResNet18)\n",
        "- **ImageNet Accuracy**: 77.3% (higher than ResNet18's 69.8%)\n",
        "- **Architecture**: MBConv blocks with Squeeze-and-Excitation (SE) attention\n",
        "\n",
        "### Model Components:\n",
        "1. **Stem**: Initial 3√ó3 convolution with stride 2\n",
        "2. **MBConv Blocks**: Mobile inverted bottleneck convolutions (7 stages)\n",
        "3. **SE Attention**: Channel attention mechanism in each block\n",
        "4. **Head**: Global average pooling + fully connected layer\n",
        "\n",
        "### Transfer Learning Approach:\n",
        "- **Pre-trained Weights**: ImageNet (1000 classes) ‚Üí Flowers102 (102 classes)\n",
        "- **Classifier Replacement**: Replace final FC layer (1000 ‚Üí 102 neurons)\n",
        "- **Two-Phase Training**: Feature extraction ‚Üí fine-tuning\n",
        "\n",
        "### Why EfficientNet-B0 for Transfer Learning?\n",
        "- **Efficiency**: Faster training and inference\n",
        "- **Generalization**: Better feature extraction despite fewer parameters\n",
        "- **Scalability**: Easy to scale up to B1-B7 if needed\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Environment Setup and Library Imports\n",
        "\n",
        "### Why This Step Matters\n",
        "Setting up the environment correctly is crucial for:\n",
        "- **Reproducibility**: Ensuring consistent results across different runs\n",
        "- **Performance**: Optimizing GPU usage and memory management  \n",
        "- **Compatibility**: EfficientNet requires specific PyTorch versions\n",
        "\n",
        "### Key Libraries Explained\n",
        "- **torch**: Core PyTorch library (tensors, automatic differentiation, neural networks)\n",
        "- **torchvision**: Computer vision utilities (datasets, transforms, pre-trained models)\n",
        "- **models**: Pre-trained model architectures (EfficientNet, ResNet, etc.)\n",
        "- **optim**: Optimization algorithms (SGD, Adam, AdamW)\n",
        "- **DataLoader**: Efficient batch processing and parallel data loading\n",
        "- **tqdm**: Progress bars for training loops\n",
        "- **matplotlib**: Data visualization and plotting\n",
        "- **sklearn**: Machine learning utilities (metrics, confusion matrix)\n",
        "\n",
        "### EfficientNet Compatibility\n",
        "EfficientNet requires PyTorch 1.6+ for proper Swish/SiLU activation support.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core PyTorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Computer vision utilities\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Machine learning utilities\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for high-quality plots\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.style.use('default')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
        "print(f\"üñºÔ∏è Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"üçé MPS available: {torch.backends.mps.is_available()}\")\n",
        "\n",
        "# Check EfficientNet availability\n",
        "try:\n",
        "    test_model = models.efficientnet_b0(pretrained=False)\n",
        "    print(\"‚úÖ EfficientNet-B0 available!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå EfficientNet not available: {e}\")\n",
        "    print(\"üí° Please update PyTorch/torchvision to latest version\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Lesson 5: EfficientNet-B0 Transfer Learning for Flower Classification\n",
        "\n",
        "## Overview\n",
        "Learn transfer learning with EfficientNet-B0, a highly efficient neural architecture that achieves excellent performance with fewer parameters than traditional CNNs. This lesson demonstrates how modern efficient architectures revolutionize the accuracy-vs-efficiency trade-off.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand EfficientNet architecture and compound scaling\n",
        "- Implement transfer learning with efficient architectures  \n",
        "- Compare efficiency metrics: accuracy vs parameters vs speed\n",
        "- Analyze the trade-offs between model complexity and performance\n",
        "\n",
        "### Model Quick Facts\n",
        "- **Architecture**: EfficientNet-B0 (efficient CNN with 5.3M parameters)\n",
        "- **Pre-training**: ImageNet dataset (1.2M images, 1000 classes)\n",
        "- **Key Innovation**: Compound scaling + MBConv blocks + SE attention\n",
        "- **Transfer Method**: Feature extraction + fine-tuning\n",
        "- **Expected Performance**: ~90%+ accuracy on Flowers102 (best yet!)\n",
        "- **Efficiency**: 2.2√ó fewer parameters than ResNet18, ~5% better accuracy\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
